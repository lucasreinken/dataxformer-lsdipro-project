{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "634877c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christophhalberstadt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/christophhalberstadt/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import collections\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import ngrams\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from functools import cache\n",
    "import cProfile\n",
    "import pstats\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9a2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('/Users/christophhalberstadt/Documents/TU Berlin/LSDIPro/tables.json', \n",
    "                    lines=True, nrows=100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01375652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config: \n",
    "\n",
    "    batch_size = 3000\n",
    "    stop_words = None \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    create_ngrams = False \n",
    "    ngram_size = 2\n",
    "    min_word_len = 0 \n",
    "\n",
    "    read_path = '/Users/christophhalberstadt/Documents/TU Berlin/LSDIPro/tables.json'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf63fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer: \n",
    "\n",
    "    def __init__(self, stemmer, stemmed_stopwords: set = None, min_word_len: int = 0): \n",
    "        self.stemmer = stemmer \n",
    "        self.stopwords = stemmed_stopwords or [] ######Hier stemmen? \n",
    "        self.min_word_len = min_word_len\n",
    "\n",
    "    def __call__(self, value:str) -> str: \n",
    "        \"\"\"\n",
    "        Hier sollte ihr DocString stehen \n",
    "        \"\"\"\n",
    "        words = self.process_words(value)\n",
    "\n",
    "        stemmed_words = []\n",
    "        for word in words: \n",
    "            stemmed_word = self.stem_cached(word)\n",
    "\n",
    "            if len(stemmed_word) >= self.min_word_len and stemmed_word not in self.stopwords: \n",
    "                stemmed_words.append(stemmed_word)\n",
    "\n",
    "        #stemmed_words = [self.stem_cached(word) for word in words if len(word)>=self.min_word_len] ####stopwords hier noch \n",
    "        if len(stemmed_words) == 0: \n",
    "            return None\n",
    "        joined_words = \" \".join(stemmed_words)\n",
    "            \n",
    "        return joined_words           \n",
    "\n",
    "    @cache\n",
    "    def stem_cached(self, word:str):\n",
    "        \"\"\"\n",
    "        Returns the stemmed version of the input word. \n",
    "\n",
    "        In: \n",
    "            Word: str\n",
    "        \n",
    "        Out: \n",
    "            The stemmed version of the word: str\n",
    "        \"\"\"\n",
    "        return self.stemmer.stem(word)\n",
    "    \n",
    "    @cache\n",
    "    def process_words(self, text:str)->list: \n",
    "        \"\"\"\n",
    "        Processes the input text into a lowercase, punctuation free and tokenized list of strings. \n",
    "\n",
    "        In: \n",
    "            text: str\n",
    "\n",
    "        Out: \n",
    "            words: list(str)\n",
    "        \"\"\"\n",
    "        sentence = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))            ####Konfigurierbar in der Init machen \n",
    "        words = word_tokenize(sentence)\n",
    "        return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9100a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_list(dfs):\n",
    "    \"\"\"\n",
    "    Creates a list of tables. \n",
    "    Indexing Rank: Table_ID -> Row_ID -> Col_ID \n",
    "\n",
    "    In: \n",
    "        dfs:pd.dataframe? \n",
    "    Out: \n",
    "        Table_List: list\n",
    "    \"\"\"\n",
    "    \n",
    "    return [[[cell.get('text', '') for cell in row] for row in table_data] for table_data in dfs['tableData']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d40014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_projections(table_list,\n",
    "                       tokenizer, \n",
    "                       ) -> dict: \n",
    "    \n",
    "\n",
    "\n",
    "    ######Trie Indexing for Fuzzy Matching fehlt \n",
    "\n",
    "    projections = dict()\n",
    "\n",
    "    for table_index, table in enumerate(table_list): \n",
    "        for row_index, row in enumerate(table): \n",
    "            for cell_index, cell in enumerate(row): \n",
    "                stemmed_cell = tokenizer(cell)\n",
    "\n",
    "                if stemmed_cell: \n",
    "                    projections.setdefault(stemmed_cell, set()).add((table_index, row_index, cell_index))\n",
    "\n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8044161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing(tokenized_value:list, projections:dict, key_id:int = None)->dict:\n",
    "    \"\"\"\n",
    "    Return a dict of all the Examples found in the projections\n",
    "    In: \n",
    "        Cleaned_Values: A List of all the Stemped Versions of one Example given\n",
    "        Projections: A Dict of Projections of all given Tables\n",
    "    Out: \n",
    "        Index_Dict: A dict of all the positions where the Example was found\n",
    "                    Form: Key: (Table_ID, Row_ID) -> Value: (Col_ID)\n",
    "    \"\"\" \n",
    "    key_id = key_id+1\n",
    "    index_dict = dict() \n",
    "    \n",
    "    value_index = projections.get(tokenized_value, None)\n",
    "\n",
    "    if value_index: \n",
    "        for table_id, row_id, col_id in value_index: \n",
    "\n",
    "            if key_id: \n",
    "                index_dict.setdefault((table_id, row_id), set()).add((key_id-1, col_id))\n",
    "            else: \n",
    "                index_dict.setdefault((table_id, row_id), set()).add((col_id))\n",
    "    \n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a6d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e70deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd28af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "table_list = create_table_list(data)\n",
    "print(len(table_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4c0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = create_projections(table_list, my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97dc1809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130\n"
     ]
    }
   ],
   "source": [
    "print(len(projections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d29651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_direct_tables(Examples:set,  \n",
    "                       projections:dict, \n",
    "                       tau: int, \n",
    "                       tokenizer:Tokenizer):\n",
    "\n",
    "    evidence = dict()\n",
    "    E = len(Examples)\n",
    "    K = None\n",
    "\n",
    "    \n",
    "\n",
    "    if tau > E: \n",
    "        raise ValueError(f\"At least Tau: {tau} examples must be given!\")\n",
    "    \n",
    "\n",
    "    for Example in Examples: \n",
    "\n",
    "        if not K: \n",
    "            K = len(Example)                \n",
    "        else: \n",
    "            if K != len(Example): \n",
    "                raise ValueError(f\"All Examples must be of the same Size!\")\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        idx_list = list()               \n",
    "        for key_id, example_key in enumerate(Example):               \n",
    "            tokens_of_example_key = tokenizer(example_key)\n",
    "\n",
    "            dict_of_idx = indexing(tokens_of_example_key, projections, key_id)  \n",
    "            idx_list.append(dict_of_idx)\n",
    "\n",
    "\n",
    "        key_sets = [set(d.keys()) for d in idx_list]\n",
    "        unique_shared_keys = set.intersection(*key_sets)\n",
    "        \n",
    "        if not unique_shared_keys: \n",
    "            continue \n",
    "\n",
    "        for key in unique_shared_keys:\n",
    "\n",
    "            all_mappings_for_this_row = []\n",
    "            for dict_of_idx in idx_list:\n",
    "\n",
    "                mappings = dict_of_idx[key] \n",
    "                all_mappings_for_this_row.append(mappings)\n",
    "\n",
    "            \n",
    "            for candidate_mapping in itertools.product(*all_mappings_for_this_row):\n",
    "\n",
    "                table_id, row_id = key\n",
    "\n",
    "                evidence.setdefault((table_id, candidate_mapping), set()).add(Example)\n",
    "\n",
    "    \n",
    "    relevant_tables = dict()\n",
    "\n",
    "    for key, value in evidence.items(): \n",
    "        if len(value) >= tau: \n",
    "            table_id, candidate_mapping = key\n",
    "            relevant_tables.setdefault(table_id, list()).append(dict(candidate_mapping))\n",
    "\n",
    "    return relevant_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9957af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_example = [('1929', 'Robert Crawford'), ('1938', 'John Patrick')]\n",
    "tau = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb61bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_example = [( 'Robert Crawford' ,'Ulster Unionist', '1929')]\n",
    "tau = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95bfe997",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tables = find_direct_tables(indexing_example, projections, tau, my_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfda2dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [{0: 2, 1: 3, 2: 1}]}\n"
     ]
    }
   ],
   "source": [
    "print(relevant_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3165c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Querries = [('Robert Crawford', 'Ulster Unionist'), ('John Patrick', 'Ulster Unionist')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2eef091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robert crawford', 'robert crawford', 'john patrick', 'john patrick', 'robert nichol wilson', 'robert simpson', 'robert simpson', 'robert simpson', 'robert simpson', 'robert simpson', 'constitu abolish']\n"
     ]
    }
   ],
   "source": [
    "table = table_list[0]\n",
    "my_list = list()\n",
    "for row in table: \n",
    "    my_list.append(my_tokenizer(row[2]))\n",
    "\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c48119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = np.array(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42b0f781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robert crawford\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "d = my_tokenizer(Querries[0][0]) \n",
    "print(d)\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfcd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "c = np.where(d == my_list, 1, 0)\n",
    "print(c)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb27bcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "<class 'int'>\n",
      "('Robert Crawford', 'Ulster Unionist')\n",
      "0\n",
      "Robert Crawford\n",
      "[]\n",
      "('John Patrick', 'Ulster Unionist')\n",
      "0\n",
      "John Patrick\n",
      "[]\n",
      "1 3\n",
      "<class 'int'>\n",
      "('Robert Crawford', 'Ulster Unionist')\n",
      "1\n",
      "Ulster Unionist\n",
      "[]\n",
      "('John Patrick', 'Ulster Unionist')\n",
      "1\n",
      "Ulster Unionist\n",
      "[]\n",
      "2 1\n",
      "<class 'int'>\n",
      "('Robert Crawford', 'Ulster Unionist')\n",
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(Ev_Col)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mEv_Col\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     21\u001b[0m tokenized_x \u001b[38;5;241m=\u001b[39m my_tokenizer(x[Ev_Col])\n\u001b[1;32m     22\u001b[0m index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(tokenized_x \u001b[38;5;241m==\u001b[39mnp_col)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "anz_querries = len(Querries)\n",
    "cols = list()\n",
    "for relevant_table, possible_mappings in relevant_tables.items(): \n",
    "    table = table_list[relevant_table]\n",
    "    for possible_mapping in possible_mappings: \n",
    "        for Ev_Col, Tab_col in possible_mapping.items(): \n",
    "            #Ev_Col, Tab_col = mapping.items()\n",
    "            tokenized_querries = []\n",
    "            print(Ev_Col, Tab_col)\n",
    "            print(type(Ev_Col))\n",
    "            col = list()\n",
    "            for row in table: \n",
    "                col.append(row[Tab_col])\n",
    "\n",
    "            np_col = np.array(col)\n",
    "            for i in range(anz_querries):\n",
    "                x = Querries[i]\n",
    "                print(x)\n",
    "                print(Ev_Col)\n",
    "                print(x[Ev_Col])\n",
    "                tokenized_x = my_tokenizer(x[Ev_Col])\n",
    "                index = np.where(tokenized_x ==np_col)[0]\n",
    "                print(index)\n",
    "            \n",
    "            cols.append(col)\n",
    "for col in cols: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "566b31de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(0, 10) -2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f64bdea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "b = np.where(a>0, 1, 0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef35d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f7c0dd8",
   "metadata": {},
   "source": [
    "IIIC: \n",
    "\n",
    "Einstieg vor/bei/nach dem Vergleich zwischen X und Y bei direct_table_join. \n",
    "Ich nehme die menge aller hier (Table_ID; Row_ID) Tuple meiner Tabelle, die dann nur ein View sind, und ziehe all die ab, die für meinen direct join genutzt werden. \n",
    "\n",
    "Ich nehme alle E.X werte dieser Tabelle und generiere eher den View (Table_ID; Col_ID): set(Row_ID). \n",
    "Wenn len(set(Row_ID)) == 1 ist, dann ist eine funktionale Beziehung zu allen Elementen in der Zeile gegeben. Also nehme ich all diese Werte als Zwischenwerte. \n",
    "\n",
    "Wenn >1, dann nehme ich mir all die Zeilen, wo E.X drin ist und itteriere über alle Spalten. Ich sammle die Elemente dort in einem set() und wenn dieses wieder len(set())==1 für eine Saplte ist, dann steht dieser Wert in einer funktional abhängigen Beziehung zu E.x.\n",
    "Wenn nicht, dann verwerfe ich diese Werte. \n",
    "\n",
    "Funktionelle Abhängigkeit wird nur für E.x geprüft, nicht für ein x in Q. \n",
    "x in Q folgen nur dem \"Pfad\", der durch E.x festgelegt wurde. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
