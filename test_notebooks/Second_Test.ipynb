{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e9afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christophhalberstadt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/christophhalberstadt/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import collections\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import ngrams\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from functools import cache\n",
    "import cProfile\n",
    "import pstats\n",
    "import time\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7c648",
   "metadata": {},
   "source": [
    "nltk stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1dc89a",
   "metadata": {},
   "source": [
    "Duck DB for View of Projctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342dc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('/Users/christophhalberstadt/Documents/TU Berlin/LSDIPro/tables.json', \n",
    "                    lines=True, nrows=10000, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a4d71",
   "metadata": {},
   "source": [
    "Real Datatables might be Row and Column Based!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf455e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config: \n",
    "\n",
    "    batch_size = 3000\n",
    "    stop_words = None \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    create_ngrams = False \n",
    "    ngram_size = 2\n",
    "    min_word_len = 0 \n",
    "\n",
    "    read_path = '/Users/christophhalberstadt/Documents/TU Berlin/LSDIPro/tables.json'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer: \n",
    "\n",
    "    def __init__(self, stemmer): \n",
    "        self.stemmer = stemmer \n",
    "\n",
    "    def __call__(self, value:str, create_ngrams:bool = False, ngram_size:int = 2) -> list: \n",
    "        \"\"\"\n",
    "        Hier sollte ihr DocString stehen \n",
    "        \"\"\"\n",
    "        ######Min_word_Len müsste hier noch enforced werden!!! \n",
    "        words = self.process_words(value)\n",
    "        stemmed_words = [self.stem_cached(word) for word in words]\n",
    "\n",
    "        if create_ngrams: \n",
    "            created_ngrams = ngrams(stemmed_words, ngram_size)\n",
    "            stemmed_words.extend(list(created_ngrams))\n",
    "            \n",
    "        return set(stemmed_words)             ####War vorher ne Liste\n",
    "\n",
    "    @cache\n",
    "    def stem_cached(self, word:str):\n",
    "        \"\"\"\n",
    "        Returns the stemmed version of the input word. \n",
    "\n",
    "        In: \n",
    "            Word: str\n",
    "        \n",
    "        Out: \n",
    "            The stemmed version of the word: str\n",
    "        \"\"\"\n",
    "        return self.stemmer.stem(word)\n",
    "    \n",
    "    @cache\n",
    "    def process_words(self, text:str)->list: \n",
    "        \"\"\"\n",
    "        Processes the input text into a lowercase, punctuation free and tokenized list of strings. \n",
    "\n",
    "        In: \n",
    "            text: str\n",
    "\n",
    "        Out: \n",
    "            words: list(str)\n",
    "        \"\"\"\n",
    "        sentence = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))            ####Konfigurierbar in der Init machen \n",
    "        words = word_tokenize(sentence)\n",
    "        return words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010bdf7",
   "metadata": {},
   "source": [
    "Die Beiden müssen noch in eine Klasse gepackt werden!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7828469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = Config.batch_size \n",
    "with open(Config.read_path, \"r\") as infile:\n",
    "    for i, line in enumerate(infile):\n",
    "        if i >= batch_size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afe055b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_json(path, csize):\n",
    "    iter_chunk = pd.read_json(path, lines=True, chunksize=csize)\n",
    "    for chunk in iter_chunk:\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372cd14",
   "metadata": {},
   "source": [
    "Bis hier!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0009330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_list(dfs):\n",
    "    \"\"\"\n",
    "    Creates a list of tables. \n",
    "    Indexing Rank: Table_ID -> Row_ID -> Col_ID \n",
    "\n",
    "    In: \n",
    "        dfs:pd.dataframe? \n",
    "    Out: \n",
    "        Table_List: list\n",
    "    \"\"\"\n",
    "    \n",
    "    return [[[cell.get('text', '') for cell in row] for row in table_data] for table_data in dfs['tableData']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4486cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_projections(table_list,\n",
    "                       #start_table_index, Warum? Ist der von mir? \n",
    "                       tokenizer, \n",
    "                       #projections,\n",
    "                       #stemmer, \n",
    "                       stop_words: list = None, \n",
    "                       min_word_len = 0, \n",
    "                       create_ngrams:bool = False, \n",
    "                       ngrams_size:int = 2\n",
    "                       ) -> dict: \n",
    "    \n",
    "\n",
    "\n",
    "    ######Trie Indexing for Fuzzy Matching fehlt \n",
    "\n",
    "    projections = dict()\n",
    "\n",
    "    #ps = stemmer\n",
    "    \n",
    "    if stop_words: \n",
    "        stemmed_stopwords = {tokenizer.stem_cached(w) for w in stop_words}\n",
    "    else:\n",
    "        stemmed_stopwords = set()\n",
    "\n",
    "    for table_index, table in enumerate(table_list): \n",
    "        for row_index, row in enumerate(table): \n",
    "            for column_index, cell in enumerate(row): \n",
    "\n",
    "                words = tokenizer.process_words(cell)\n",
    "\n",
    "                if create_ngrams: \n",
    "                    ngram_stem_set = set()\n",
    "\n",
    "                for word in words: \n",
    "                    stemmed_word = tokenizer.stem_cached(word)\n",
    "                        \n",
    "                    if stemmed_word not in stemmed_stopwords and len(stemmed_word) >= min_word_len: \n",
    "\n",
    "                        projections.setdefault(stemmed_word, set()).add((table_index, row_index, column_index))\n",
    "\n",
    "                        if create_ngrams: \n",
    "                            ngram_stem_set.update(stemmed_word)\n",
    "                \n",
    "                if create_ngrams: \n",
    "                    created_ngrams = ngrams(ngram_stem_set, ngrams_size)\n",
    "                    for created_ngram in created_ngrams: \n",
    "                        projections.setdefault(created_ngram, set()).add((table_index, row_index, column_index))\n",
    "    \n",
    "    return projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf137d",
   "metadata": {},
   "source": [
    "Also: \n",
    "In: E_Set\n",
    "\n",
    "Relevant_Tables = dict()\n",
    "\n",
    "for E in E_Set: \n",
    "    X, Y = E \n",
    "    for X get all (Table_ID, Row_ID, Col_ID)\n",
    "    for Y get all (Table_ID, Row_ID, Col_ID)\n",
    "\n",
    "\n",
    "    for all ID_Tup in X: \n",
    "        for all ID_Tup2 in Y: \n",
    "            if Table_ID_X == Table_ID_Y and Row_ID_X == Row_ID_Y and Col_ID_X != Col_ID_Y: \n",
    "                relevant_Tables.setdefault(Table_ID): (Row_ID, (Col_ID_X, Col_ID_Y))\n",
    "\n",
    "\n",
    "Probleme hiermit: \n",
    "    X und Y können Subkeys sein. \n",
    "    Dabei kann X über zwei splaten definiert sein und: \n",
    "        X_i in X kann durch Tokenizer zu einer Liste werden!! \n",
    "\n",
    "        Also mehr info needed!!!! \n",
    "\n",
    "\n",
    "\n",
    "Zusammengesetzte Keys: \n",
    "Weenn X oder Y ein zusammengesetzer Key, also \n",
    "    if isinstance(X, tuple) oder if isinstance(Y, tuple): \n",
    "        for x_i/ y_i in X/Y get all (Table_ID, Row_ID, Col_ID)\n",
    "\n",
    "Es gilt immer noch!!! Forall x_i in X and y_i in Y. \n",
    "To be relevant, forall x_i in X, y_i in Y, Table_ID: Same, Row_ID: Same, Col_ID:Different!!! \n",
    "\n",
    "-> Kein Problem, selbe Struktur wie oben, aber es müssen mehr Vergleiche Stattfinden. \n",
    "-> Early Opt out to kill Table? \n",
    "-> Sorting musst start from big to small, so first compare Table_ID (fast)\n",
    "-> Then compare Row_ID \n",
    "->Then compare Col_ID\n",
    "if something is wrong, continue with next \n",
    "\n",
    "\n",
    "Zusammengesetzte Values: \n",
    "z.B. Fc Bayern München. -> Fc, Bayern, München \n",
    "Alle sollten im selben Column sein. \n",
    "Wenn nicht, dann müsste man die Teil Columns Mergen. \n",
    "Wie macht man so eine Struktur? \n",
    "Was ist mit der Informationsqualität. Eg. Fc Bayern München mit min_word_size und zusammengesetzt eindeutig -> Fussballverein. \n",
    "                                            Bayern München mit min_word_size = 3 und zusammengsetzt -> Liste aller Länder und deren Landeshauptstädte in DE?? \n",
    "                                            Liste mit min_word_size = 0 und NICHT zusammengesetzt uneindeutig -> Fc Köln != Fc Bayern München, aber Fc = Fc, also Hit \n",
    "                                            Daher ngrams? Wahrscheinlich schon. Fc Bayern eher eindeutig, wenn auch nicht perfekt. Alle ngrams bis 3, also Fc, Bayern, München, Fc Bayern, Bayern München, Fc Bayern München hittet viel (auch unnötiges) aber definitiv auch das richtige. Würde das nicht Fc, Bayern, München auch? Bringen ngrams hier was? hmmmm \n",
    "                    \n",
    "Wenn drei Einzelne, dann wird Table_ID, Col_ID, Row_ID mindestens drei mal gehittet. \n",
    "Da aber set(), egal!!! \n",
    "Sagen wir, mir ist egal wie mein E aussieht. Ich tokenize alles in listen und hänge die Liste zusammen. Dann müssen einfach, wenn ich n X und m Y habe nach dem Tokenizen mindestens len(Liste) >= n+m sein und damit Table zählt müssen mindestens n+m Hits in Zeile sein. \n",
    "Für Tabelle dann Tau Zeilen Hits! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Also, da mir die einzelnen Wörter total egal sind, ist das Token selbst info, die ich verlieren kann. \n",
    "Was ich jetzt brauche ist nur Info: \n",
    "Anzahl Subkeys X, Anzahl Subkeys Y, Tokenized Subkeys liste/set? hmmm. set()!! Wenn nach Tokenize die Anzahl kleiner ist, dann muss ich entweder das als minimal Requirement nutzen oder das Example Error Raisen!!! \n",
    "(Table_ID, Row_ID, Col_ID) pro Val in Tokenized_Example_Set \n",
    "Welche Datenstruktur wäre am schnellsten für den Vergleich? \n",
    "Ah, nächstes Problem. \n",
    "Da ja l ID_Tuples pro Val in Tokenized_Example_List auftauchen können, muss auch getracked werden, zu welchem die gehören. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Neue Datenstruktur: \n",
    "\n",
    "Example_Dict: Dict()\n",
    "for Example in Example_Dict: \n",
    "\n",
    "Table_ID, Row_ID, Col_ID\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Besser: list(Dict_1; ..., Dict_(n+m))\n",
    "Dict_Structure: (Table_ID, Row_ID): Col_ID\n",
    "\n",
    "Compare_Keys = Set(Dict_1) & Set(Dict_2) & ... Set(Dict_(n+m))\n",
    "If Compare_Keys: \n",
    "    for key in Compare_Keys: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Die Anzahl der Col muss nur n+m sein, um einen Example_Hit zu verzeichnen.\n",
    "Mir können duplikate in zwei columns total egal sein, jedes Dict enthält ein Set. \n",
    "Ich muss alle gleichen Keys finden und nur alle sets joinen. Wenn die kardinalität des Sets n+m entspricht ist es ein Hit, sonst ein Miss \n",
    "\n",
    "Die einzige Frage: Was returne ich? Da ich das ja anpassen kann wie ich will ist mir das erstmal egal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a91fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing(tokenized_values:list, projections:dict)->dict:\n",
    "    \"\"\"\n",
    "    Return a dict of all the Examples found in the projections\n",
    "    In: \n",
    "        Cleaned_Values: A List of all the Stemped Versions of one Example given\n",
    "        Projections: A Dict of Projections of all given Tables\n",
    "    Out: \n",
    "        Index_Dict: A dict of all the positions where the Example was found\n",
    "                    Form: Key: (Table_ID, Row_ID) -> Value: (Col_ID)\n",
    "    \"\"\" \n",
    "\n",
    "    index_dict = dict() \n",
    "    \n",
    "    for value in tokenized_values: \n",
    "        value_index = projections.get(value, None)\n",
    "\n",
    "        if value_index: \n",
    "            for table_id, row_id, col_id in value_index: \n",
    "                #table_id, row_id, col_id = index_pair \n",
    "                index_dict.setdefault((table_id, row_id), set()).add((col_id))\n",
    "    \n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d68db63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_direct_tables(Examples:set,  \n",
    "                       projections:dict, \n",
    "                       Tau: int, \n",
    "                       tokenizer:Tokenizer):\n",
    "\n",
    "    table_dict = dict() \n",
    "    E = len(Examples)\n",
    "    K = None\n",
    "\n",
    "\n",
    "    if Tau > E: \n",
    "        raise ValueError(f\"At least Tau: {Tau} examples must be given!\")\n",
    "    \n",
    "\n",
    "    for Example in Examples: \n",
    "\n",
    "        if not K: \n",
    "            K = len(Example)                \n",
    "        else: \n",
    "            if K != len(Example): \n",
    "                raise ValueError(f\"All Examples must be of the same Size!\")\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        idx_list = list()               #####Speichert die dict_of_idx pro example_key          ###Hier dict für x und y? \n",
    "        for example_key in Example:                 #####Vorher hier in X und Y spalten? \n",
    "            tokens_of_example_key = tokenizer(example_key)\n",
    "\n",
    "            dict_of_idx = indexing(tokens_of_example_key, projections)\n",
    "            idx_list.append(dict_of_idx)\n",
    "\n",
    "\n",
    "        key_sets = [set(d.keys()) for d in idx_list]\n",
    "        unique_shared_keys = set.intersection(*key_sets)\n",
    "        \n",
    "        if not unique_shared_keys: \n",
    "            continue \n",
    "\n",
    "\n",
    "        for unique_shared_key in unique_shared_keys: \n",
    "            col_set = set()\n",
    "            valid = True\n",
    "            for dict_of_idx in idx_list: \n",
    "                cols = dict_of_idx.get(unique_shared_key, None)\n",
    "                if not cols: \n",
    "                    valid = False \n",
    "                    break\n",
    "                else: \n",
    "                    col_set = col_set.union(cols)\n",
    "            \n",
    "            if valid == True and len(col_set) >= K: ######zweiter Teil dieser Bedingung muss aufgeweicht werden!!! \n",
    "                Table_ID, Row_ID = unique_shared_key\n",
    "                table_dict.setdefault((Table_ID, *list(col_set)), set()).add(Row_ID)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    relevant_tables = set()\n",
    "\n",
    "    for key in table_dict: \n",
    "        elems = table_dict.get(key)\n",
    "        if len(elems) >= Tau: \n",
    "            relevant_tables.add(key)\n",
    "    \"\"\"\n",
    "\n",
    "    relevant_tables = dict()\n",
    "\n",
    "    for key in table_dict: \n",
    "        elems = table_dict.get(key)\n",
    "        if len(elems) >= Tau: \n",
    "            relevant_tables.setdefault(key[0], key[1:])\n",
    "    \n",
    "    return relevant_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_direct_tables(Examples:dict,  \n",
    "                       projections:dict, \n",
    "                       Tau: int, \n",
    "                       tokenizer:Tokenizer):\n",
    "\n",
    "    table_dict = dict() \n",
    "    E = len(Examples)\n",
    "    K = None\n",
    "\n",
    "\n",
    "    if Tau > E: \n",
    "        raise ValueError(f\"At least Tau: {Tau} examples must be given!\")\n",
    "    \n",
    "\n",
    "    for Example in Examples: \n",
    "\n",
    "        #####Nehmen wir an, Example wäre ein dict: x:{key1, ..., keyn}, y:{key1, ..., keym}\n",
    "\n",
    "        if not K: \n",
    "            K = len(Example)                \n",
    "        else: \n",
    "            if K != len(Example): \n",
    "                raise ValueError(f\"All Examples must be of the same Size!\")\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        idx_list = dict               #####Speichert die dict_of_idx pro example_key          ###Hier dict für x und y? \n",
    "        for side, keys_per_side in Example:                 #####Vorher hier in X und Y spalten? \n",
    "            for example_key in keys_per_side: \n",
    "\n",
    "                tokens_of_example_key = tokenizer(example_key)\n",
    "\n",
    "                len_of_tokens = len(tokens_of_example_key)\n",
    "                #####A:\n",
    "                #####Hier können zwei verschiedene Ergeinisse auftreten: \n",
    "                #####1: |tokens_of_example_key| = 1\n",
    "                #####2: |tokens_of_example_key| > 1\n",
    "                #####Können leere Tokens auftreten? \n",
    "                #####Da min_word_len nicht im tokenizer enforced wird, können hier auch fehlerhafte Tokens sein, die niemals in dict auftauchen werden\n",
    "                #####zB of oder fc sollten bei min_word_len=3 oder of in Stopwords gekilled werden. Key of wäre auch dumm, aber Tubsoken possible \n",
    "                #####min_word_len im Tokenizer enforcen? \n",
    "\n",
    "                dict_of_idx = indexing(tokens_of_example_key, projections)\n",
    "\n",
    "                ####B:\n",
    "                ####Hier Können drei verschiedene Ereignisse auftreten: \n",
    "                ####1: dict_of_idx ist leer -> Break Example \n",
    "                ####2: für ein (Table_ID, Row_ID) ist len() = 1\n",
    "                ####3: für ein (Table_ID, Row_ID) ist len() > 1 \n",
    "\n",
    "                ####Was ist, wenn dict_of_idx leer ist? Break example? \n",
    "                ####Wenn A = 1 und B = 1 -> Eindeutig\n",
    "                ####Wenn A = 1 und B > 1 -> Uneindeutig, Problem!! Aber, kann durch andere Keys und Examples gefiltert werden\n",
    "                ####Wenn A > 1 und B = 1 -> Treffer, aber sind alle tokens in der einer Spalte? Ist es ein Problem, wenn dem nicht so ist? \n",
    "                ####Wenn A > 1 und B > 1 -> Key ist auf mehrere Spalten verteilt? Sind alle Tokens ein Hit? Problem, wenn dem nicht so ist? \n",
    "\n",
    "\n",
    "                ####Brauche ich einen Faktor Precission zusätzlich zu Tau? Sortiert sich das selber durch Tau und weitere Examples aus? \n",
    "\n",
    "                idx_list.setdefault(side, list()).append(dict_of_idx)       ####Also für jeden Subkey einer Seite alle seine Beispiele \n",
    "\n",
    "\n",
    "        key_sets = [set(d.keys()) for d in idx_list]\n",
    "        unique_shared_keys = set.intersection(*key_sets)\n",
    "        \n",
    "        if not unique_shared_keys: \n",
    "            continue \n",
    "\n",
    "\n",
    "        for unique_shared_key in unique_shared_keys: \n",
    "            col_set = set()\n",
    "            valid = True\n",
    "            for dict_of_idx in idx_list: \n",
    "                cols = dict_of_idx.get(unique_shared_key, None)\n",
    "                if not cols: \n",
    "                    valid = False \n",
    "                    break\n",
    "                else: \n",
    "                    col_set = col_set.union(cols)\n",
    "            \n",
    "            if valid == True and len(col_set) >= K: #######Ist hier der zweite Vergleich wirklich war? schließlich könnten ja auch zwei Keys in einer Col sein\n",
    "                                                    #######Was ist die mindest Menge an Hits? Reicht Condition 1, dass jeder Key einen Hit \n",
    "                                                    #######Verzeichnet haben muss?\n",
    "                                                    #######Multi Hits eines Keys mit len(Tokens) > 1 als ein Tuple? \n",
    "                                                    #######Müsste für unverschateltem Readout wieder aufgespalten Werden. \n",
    "                                                    #######Eher Struktur dict Key: x Value: dict Key: side_key Value: cols???\n",
    "                Table_ID, Row_ID = unique_shared_key\n",
    "                table_dict.setdefault((Table_ID, *list(col_set)), set()).add(Row_ID)\n",
    "\n",
    "\n",
    "    \n",
    "    relevant_tables = set()\n",
    "\n",
    "    for key in table_dict: \n",
    "        elems = table_dict.get(key)\n",
    "        if len(elems) >= Tau: \n",
    "            relevant_tables.add(key)\n",
    "    \n",
    "    return relevant_tables\n",
    "\n",
    "\n",
    "####Output Information sollte die Anzahl an Example Hits Tau beinhalten. Also Dict in Dict Structure, mit den Keys: Cols, Hits, Rows? \n",
    "\n",
    "####Also, welche Informationen brauche ich für die Evaluation? \n",
    "####Die Anzahl an Hits pro Example einer Tabelle. -> Table based -> Hits/ Examples \n",
    "####\n",
    "####Welche Information brauche ich für einen Example hit? \n",
    "####Alle/ die meisten Keys müssen einen oder mehrere Hits verzeichnen /Mehrere Hits müssen zu einem werden. \n",
    "####Brauche ich die Information welches Token wo getroffen hat, um eine Trennung des Keys im allgemeinen zu bestimmen? \n",
    "####Also für die nachfolgdenden Querries!\n",
    "####\n",
    "####Wenn zwei substantiell unterschiedliche Keys einer Seite in einer Spalte einen Hit landen, dann müssen beide Keys einen Hit verzeichnen\n",
    "####Aka, Hits = K (Errechnung von K muss geändert werden)\n",
    "####Wenn |Tokens| > 1 für einen Key, wie viele müssen Hitten um einen Hit für das Example zu bedeuten? \n",
    "####zB |Tokens| = 2, und zwei Spalten, fine. Beide Hits in einer Saplte, Fine, Nur ein Hit in einer Spalte? -> Luca fragen\n",
    "####zB |Tokens| = 1, zwei Spalten. Nehme ich beide auf und verzeichne nur einen Hit, in der Hoffunung, dass weitere Examples das Problem lösen? \n",
    "####\n",
    "####Alternative, der bisherige Ansatz, selbst ohne x und y Trennung und ich löse das Problem einfach in einem zweiten, kleineren, refinement Schritt\n",
    "####Dort ordne ich jedem Key die Spalte zu und gebe so die Lösung für eine bestimmte Tabelle an?? \n",
    "####Logik auf kleiner endlicher Anzahl an Tabellen mit kleiner endlicher Anzahl an Examples ist nicht besonders hart. \n",
    "####Aber!!! Bound für Tabelle ist deutlich leichter!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a7577",
   "metadata": {},
   "source": [
    "Nur ein Eintrag für alle Subtokens, egal wie viele es sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15c625d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "table_list = create_table_list(data)\n",
    "print(len(table_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2083b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer(Config.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ebdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = create_projections(table_list, my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8da7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_example = [('1929', 'Robert Crawford'), ('1938', 'John Patrick')]\n",
    "tau = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5577cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tables = find_direct_tables(indexing_example, projections, tau, my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d598d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(relevant_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1929', '1933', '1938', '1945', '1949', '1953', '1958', '1962', '1965', '1969', '1973']\n",
      "['Robert Crawford', 'Robert Crawford', 'John Patrick', 'John Patrick', 'Robert Nichol Wilson', 'Robert Simpson', 'Robert Simpson', 'Robert Simpson', 'Robert Simpson', 'Robert Simpson', 'Constituency abolished']\n"
     ]
    }
   ],
   "source": [
    "col1 = [] \n",
    "col2 = []\n",
    "table_id, x_col_id, y_col_id = next(iter(relevant_tables))          ######Lieber Col Store für die Speicherung der Tabelle? \n",
    "                                                                    ######Ist \"richtiger\"/schneller\n",
    "for row in table_list[table_id]: \n",
    "    col1.append(row[x_col_id])\n",
    "    col2.append(row[y_col_id])\n",
    "\n",
    "print(col1)\n",
    "print(col2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408461c2",
   "metadata": {},
   "source": [
    "Lieber einfach alle Cols auswählen, die identifiziert wurden und auf diesen das Mapping suchen? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
